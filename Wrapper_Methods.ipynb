{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Wrapper methods\n"
      ],
      "metadata": {
        "id": "HpzZKHwQrvYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Suitable for Classification/Regression\n"
      ],
      "metadata": {
        "id": "-9HxRf1wSQC2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove Correlated features\n",
        "\n",
        "Step Forward & Backward Feature Selection takes a long time to run, so to speed it up we will reduce the feature space by removing correlated features first."
      ],
      "metadata": {
        "id": "u7qSNKV_SUYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Define a function to find correlated features\n",
        "# This line defines a function 'correlation' that takes a dataset and a threshold as inputs.\n",
        "def correlation(dataset, threshold):\n",
        "\n",
        "    # Step 2: Initialize an empty set to store correlated column names\n",
        "    # Create an empty set 'col_corr' to hold the names of correlated columns.\n",
        "    col_corr = set()\n",
        "\n",
        "    # Step 3: Compute the correlation matrix of the dataset\n",
        "    # Calculate the correlation matrix for all numeric features in the dataset.\n",
        "    corr_matrix = dataset.corr()\n",
        "\n",
        "    # Step 4: Loop over each column in the correlation matrix\n",
        "    # Iterate through the columns of the correlation matrix by their index.\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "\n",
        "        # Step 5: Loop over the previous columns\n",
        "        # For each column, iterate over all columns before it (to avoid repetition).\n",
        "        for j in range(i):\n",
        "\n",
        "            # Step 6: Check if the absolute value of correlation exceeds the threshold\n",
        "            # If the correlation between two columns is greater than the threshold, consider them correlated.\n",
        "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
        "\n",
        "                # Step 7: Get the name of the correlated column\n",
        "                # Store the name of the correlated column in the 'colname' variable.\n",
        "                colname = corr_matrix.columns[i]\n",
        "\n",
        "                # Step 8: Add the correlated column name to the set\n",
        "                # Add the column name to the set 'col_corr' to avoid duplicates.\n",
        "                col_corr.add(colname)\n",
        "\n",
        "    # Step 9: Return the set of correlated features\n",
        "    # The function returns the set of correlated column names.\n",
        "    return col_corr\n",
        "\n",
        "# Step 10: Call the correlation function on the training data with a threshold of 0.8\n",
        "# 'corr_features' stores the names of columns in 'X_train' that have correlations greater than 0.8.\n",
        "corr_features = correlation(X_train, 0.8)\n",
        "\n",
        "# Step 11: Print the number of correlated features\n",
        "# Display the count of correlated features that will be removed.\n",
        "print('correlated features: ', len(set(corr_features)))\n",
        "\n",
        "# Step 12: Remove the correlated features from the training set\n",
        "# Drop the correlated features from 'X_train'.\n",
        "X_train.drop(labels=corr_features, axis=1, inplace=True)\n",
        "\n",
        "# Step 13: Remove the correlated features from the test set\n",
        "# Drop the same correlated features from 'X_test'.\n",
        "X_test.drop(labels=corr_features, axis=1, inplace=True)\n",
        "\n",
        "# Step 14: Check the shapes of the training and test sets after feature removal\n",
        "# Output the new dimensions of 'X_train' and 'X_test'.\n",
        "X_train.shape, X_test.shape"
      ],
      "metadata": {
        "id": "3_t9ilGMSdug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward Feature Selection\n"
      ],
      "metadata": {
        "id": "agUtxejcSqds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step forward feature selection** starts by training a machine learning model for each feature in the dataset and selecting, as the starting feature, the one that returns the best performing model according to the evaluation criteria we choose.\n",
        "\n",
        "**In the second step**, it creates machine learning models for all combinations of the feature selected in the previous step and a second feature. It selects the pair that produces the best performing algorithm.\n",
        "\n",
        "It continues by adding 1 feature at a time to the features that were selected in previous steps, until a stopping criteria is reached.\n",
        "\n",
        "Step forward feature selection is called a greedy procedure because it evaluates many possible single, double, triple, and so on feature combinations. **Therefore, it is very computationally expensive and, sometimes, if the feature space is big enough, even unfeasible.**"
      ],
      "metadata": {
        "id": "fdpDCsSrbKjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.feature_selection import SequentialFeatureSelector as SFS"
      ],
      "metadata": {
        "id": "rCX6kJ9arvR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Set up Sequential Feature Selector (SFS)\n",
        "# Define the SFS with a RandomForestClassifier and its parameters.\n",
        "sfs = SFS(\n",
        "    estimator=RandomForestClassifier(n_estimators=10, n_jobs=4, random_state=0),  # RandomForest with 10 trees and parallel processing\n",
        "    n_features_to_select=10,  # Number of features to retain\n",
        "    tol=None,  # Stopping criteria (None to ignore performance change)\n",
        "    direction='forward',  # Step forward selection\n",
        "    scoring='roc_auc',  # Use roc_auc as the evaluation metric\n",
        "    cv=2,  # 2-fold cross-validation\n",
        "    n_jobs=4,  # Enable parallel processing with 4 jobs\n",
        ")\n",
        "\n",
        "# Step 2: Fit the SFS model on the training data\n",
        "# Train the model using 'X_train' and 'y_train'.\n",
        "sfs = sfs.fit(X_train, y_train)\n",
        "\n",
        "# Step 3: Get the selected feature names\n",
        "# Retrieve and store the names of the selected features.\n",
        "selected_feat = sfs.get_feature_names_out()\n",
        "\n",
        "# Step 4: Output the selected features\n",
        "# Display the list of selected features.\n",
        "selected_feat"
      ],
      "metadata": {
        "id": "IA3pyQc0Robl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backward Feature Selection"
      ],
      "metadata": {
        "id": "XRUX-_Ovaggh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step Backward Feature Selection starts by fitting a model using all the features in the data set and determining its performance.\n",
        "\n",
        "* Then, it trains models on all possible combinations of all features, minus one, and removes the feature that returns the model with the lowest performance.\n",
        "* In the third step, it trains models in all possible combinations of the features remaining from step 2, minus 1 feature, and removes the feature that produced the lowest performing model.\n",
        "* The algorithm stops when a certain criteria determined by the user is met. This criteria could be that the model performance does not decrease beyond a certain threshold, or alternatively, as in the mlxtend implementation, when we reach a certain number of selected features.\n",
        "\n",
        "Step Backward Feature Selection is called greedy because it evaluates all possible n, and then n-1 and n-2 and so on feature combinations. Therefore, it is very computationally expensive and sometimes, if the feature space is big enough, even unfeasible."
      ],
      "metadata": {
        "id": "CYbiFWNqbQiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change this parameter\n",
        "direction='forward',  # Step forward selection"
      ],
      "metadata": {
        "id": "DqqlxpW4anu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exhaustive Search"
      ],
      "metadata": {
        "id": "iLcVwKTqblOv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exhaustive Feature Selection finds the best subset of features out of all possible subsets, according to a determined performance metric for a certain machine learning algorithm.\n",
        "\n",
        "Exhaustive Feature Selection is a greedy algorithm as it evaluates all possible feature combinations. It is very computationally expensive and, sometimes, if the feature space is large, even unfeasible.\n",
        "\n",
        "**Due to not being practical in many cases, i will not include a code for this algorithm**"
      ],
      "metadata": {
        "id": "iECTHLoDdcGO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare performance of feature subsets\n"
      ],
      "metadata": {
        "id": "IrTq2GaXThz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Define a function to train and evaluate Random Forests\n",
        "# This function takes training and test datasets as inputs and evaluates their performance.\n",
        "def run_randomForests(X_train, X_test, y_train, y_test):\n",
        "\n",
        "    # Step 2: Initialize the RandomForestClassifier\n",
        "    # Set up a Random Forest model with 200 trees and a maximum depth of 4.\n",
        "    rf = RandomForestClassifier(n_estimators=200, random_state=39, max_depth=4)\n",
        "\n",
        "    # Step 3: Fit the Random Forest model to the training data\n",
        "    # Train the model using 'X_train' and 'y_train'.\n",
        "    rf.fit(X_train, y_train)\n",
        "\n",
        "    # Step 4: Evaluate the model on the training set\n",
        "    # Predict probabilities for the training data and print roc-auc score.\n",
        "    print('Train set')\n",
        "    pred = rf.predict_proba(X_train)\n",
        "    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
        "\n",
        "    # Step 5: Evaluate the model on the test set\n",
        "    # Predict probabilities for the test data and print roc-auc score.\n",
        "    print('Test set')\n",
        "    pred = rf.predict_proba(X_test)\n",
        "    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
      ],
      "metadata": {
        "id": "eptngLywTi_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Evaluate the Random Forest performance with selected features\n",
        "# Run the 'run_randomForests' function using only the selected features from 'X_train' and 'X_test'.\n",
        "run_randomForests(X_train[selected_feat],\n",
        "                  X_test[selected_feat],\n",
        "                  y_train, y_test)\n",
        "\n",
        "# Step 2: Compare the performance with all features\n",
        "# Run the 'run_randomForests' function using all features in 'X_train' and 'X_test' (excluding previously removed correlated features).\n",
        "run_randomForests(X_train,\n",
        "                  X_test,\n",
        "                  y_train, y_test)"
      ],
      "metadata": {
        "id": "34JJN1rPTmiW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}