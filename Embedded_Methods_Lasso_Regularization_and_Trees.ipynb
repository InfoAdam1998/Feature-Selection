{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lasso regularisation\n"
      ],
      "metadata": {
        "id": "CWrjlKiCrla2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularisation consists in adding a penalty to the different parameters of the machine learning model to reduce the freedom of the model and avoid overfitting. In linear model regularization, the penalty is applied to the coefficients that multiply each of the predictors. The Lasso regularization or l1 has the property that is able to shrink some of the coefficients to zero. Therefore, those features can be removed from the model."
      ],
      "metadata": {
        "id": "so605vxlrqy2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsznSf-jZrlk"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.linear_model import Lasso, LogisticRegression\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification\n"
      ],
      "metadata": {
        "id": "MVjx7G1ortPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Initialize the Logistic Regression model for feature selection\n",
        "# This sets up the model to use L1 regularization (Lasso) to select features.\n",
        "sel_ = SelectFromModel(\n",
        "    LogisticRegression(C=0.5, penalty='l1', solver='liblinear', random_state=10))\n",
        "\n",
        "# Step 2: Fit the model to the training data\n",
        "# This trains the model on the scaled training data and selects important features.\n",
        "sel_.fit(scaler.transform(X_train), y_train)\n",
        "\n",
        "# Step 3: Get the indices of the selected features\n",
        "# This returns a boolean array indicating which features were selected.\n",
        "sel_.get_support()\n",
        "\n",
        "# Step 4: Create a list of selected features\n",
        "# This extracts the names of the features that were selected by the model.\n",
        "selected_feat = X_train.columns[(sel_.get_support())]\n",
        "\n",
        "# Step 5: Print total number of features\n",
        "# This shows how many features were present in the original training set.\n",
        "print('total features: {}'.format((X_train.shape[1])))\n",
        "\n",
        "# Step 6: Print number of selected features\n",
        "# This displays how many features were selected by the Lasso model.\n",
        "print('selected features: {}'.format(len(selected_feat)))\n",
        "\n",
        "# Step 7: Count features with coefficients shrunk to zero\n",
        "# This counts how many coefficients are exactly zero after fitting the model.\n",
        "print('features with coefficients shrank to zero: {}'.format(\n",
        "    np.sum(sel_.estimator_.coef_ == 0)))\n",
        "\n",
        "# Step 8: Get the number of features with coefficients shrunk to zero\n",
        "# This retrieves the count of coefficients that are zero.\n",
        "np.sum(sel_.estimator_.coef_ == 0)\n",
        "\n",
        "# Step 9: Identify the removed features\n",
        "# This creates a list of features that were not selected (coefficients equal to zero).\n",
        "removed_feats = X_train.columns[(sel_.estimator_.coef_ == 0).ravel().tolist()]\n",
        "removed_feats\n",
        "\n",
        "# Step 10: Transform the training set to include only selected features\n",
        "# This creates a new training set with only the selected features.\n",
        "X_train_selected = sel_.transform(X_train)\n",
        "\n",
        "# Step 11: Transform the test set to include only selected features\n",
        "# This creates a new test set with only the selected features.\n",
        "X_test_selected = sel_.transform(X_test)\n",
        "\n",
        "# Step 12: Check the shapes of the new training and test sets\n",
        "# This shows the dimensions of the transformed training and test sets.\n",
        "X_train_selected.shape, X_test_selected.shape"
      ],
      "metadata": {
        "id": "wGlbxm7RrtqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression"
      ],
      "metadata": {
        "id": "EQ9YE_DasXmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Train a Lasso Linear Regression model for feature selection\n",
        "# This uses Lasso, which allows for regularization to shrink some coefficients to zero.\n",
        "sel_ = SelectFromModel(Lasso(alpha=100, random_state=10))\n",
        "\n",
        "# Step 2: Fit the model to the training data\n",
        "# This trains the Lasso model on the scaled training data and selects important features.\n",
        "sel_.fit(scaler.transform(X_train), y_train)\n",
        "\n",
        "# Step 3: Get the indices of the selected features\n",
        "# This returns a boolean array indicating which features were selected.\n",
        "sel_.get_support()\n",
        "\n",
        "# Step 4: Create a list of selected features\n",
        "# This extracts the names of the features that were selected by the Lasso model.\n",
        "selected_feat = X_train.columns[(sel_.get_support())]\n",
        "\n",
        "# Step 5: Print total number of features\n",
        "# This shows how many features were present in the original training set.\n",
        "print('total features: {}'.format((X_train.shape[1])))\n",
        "\n",
        "# Step 6: Print number of selected features\n",
        "# This displays how many features were selected by the Lasso model.\n",
        "print('selected features: {}'.format(len(selected_feat)))\n",
        "\n",
        "# Step 7: Count features with coefficients shrunk to zero\n",
        "# This counts how many coefficients are exactly zero after fitting the model.\n",
        "print('features with coefficients shrank to zero: {}'.format(\n",
        "    np.sum(sel_.estimator_.coef_ == 0)))"
      ],
      "metadata": {
        "id": "H78L2fcir04t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest importance\n"
      ],
      "metadata": {
        "id": "6HavvxSJxNcB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forests is one the most popular machine learning algorithms. It is so successful because it provide good predictive performance, **low overfitting and easy interpretability**. This interpretability is given by the fact that it is straightforward to derive the importance of each variable on the tree decision. In other words, it is easy to compute how much each variable is contributing to the decision.\n",
        "\n",
        "**Procedure**\n",
        "\n",
        "Random forests consist typically of 4-12 hundred decision trees, each of them built over a random extraction of the observations from the dataset and a random extraction of the features. Not every tree sees all the features or all the observations, and this guarantees that the trees are de-correlated and therefore less prone to over-fitting. Each tree is also a sequence of yes-no questions based on a single or a combination of features. At each node (that is, at each question), the three divides the dataset in 2 buckets, each of them hosting observations that are more similar among themselves and different from the ones in the other bucket. Therefore, the importance of each feature is derived by how \"pure\" each of the buckets is.\n",
        "\n",
        "For classification, the measure of impurity is either Gini or the entropy. For regression the measure of impurity is the variance. When training a tree, it is possible to compute how much each feature decreases the impurity. The more a feature decreases the impurity, the more important the feature is. In random forests, the impurity decrease elicited by each feature is averaged across trees to determine the final importance of the variable.\n",
        "\n",
        "In general, features that are selected at the top of the trees are more important than features that are selected at the end nodes of the trees, as generally the top splits lead to bigger information gains.\n",
        "\n",
        "**Note**\n",
        "\n",
        "* Random Forests and decision trees in general give preference to features with high cardinality\n",
        "* Correlated features will be given equal or similar importance, but overall reduced importance compared to the same tree built without correlated counterparts.\n",
        "\n",
        "**Pros:**\n",
        "* Fast and easy to compute.\n",
        "* Provides a clear ranking of feature importance.\n",
        "* Works well with large datasets and high-dimensional data.\n",
        "\n",
        "**Cons:**\n",
        "* It can be biased towards features with more levels (e.g., continuous variables might appear more important than binary ones).\n",
        "* Does not explicitly eliminate features; you need to select an importance threshold yourself to reduce features."
      ],
      "metadata": {
        "id": "jlD-XBFp1HsI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification"
      ],
      "metadata": {
        "id": "kN9B86og5No5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries for Random Forest and feature selection\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# Step 2: Initialize the SelectFromModel with Random Forest Classifier\n",
        "# This sets up the model to select features based on their importance.\n",
        "sel_ = SelectFromModel(RandomForestClassifier(n_estimators=10, random_state=10))\n",
        "\n",
        "# Step 3: Fit the model to the training data\n",
        "# This trains the Random Forest on the training set and selects important features.\n",
        "sel_.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Get the indices of the selected features\n",
        "# This returns a boolean array indicating which features were selected.\n",
        "sel_.get_support()\n",
        "\n",
        "# Step 5: Create a list of selected features\n",
        "# This extracts the names of the features that were selected by the model.\n",
        "selected_feat = X_train.columns[(sel_.get_support())]\n",
        "\n",
        "# Step 6: Count the number of selected features\n",
        "# This shows how many features were deemed important by the model.\n",
        "len(selected_feat)\n",
        "\n",
        "# Step 7: Display the selected features\n",
        "# This lists the names of the features that were selected.\n",
        "selected_feat\n",
        "\n",
        "# Step 8: Plot the distribution of feature importances\n",
        "# This visualizes the importance values of all features.\n",
        "pd.Series(sel_.estimator_.feature_importances_.ravel()).hist(bins=20)\n",
        "plt.xlabel('Feature importance')\n",
        "plt.ylabel('Number of Features')\n",
        "plt.show()\n",
        "\n",
        "# Step 9: Compare the number of selected features with features above the mean importance\n",
        "# This checks the total number of features, selected features, and those above average importance.\n",
        "print('total features: {}'.format((X_train.shape[1])))\n",
        "\n",
        "print('selected features: {}'.format(len(selected_feat)))\n",
        "\n",
        "print(\n",
        "    'features with importance greater than the mean importance of all features: {}'.format(\n",
        "        np.sum(sel_.estimator_.feature_importances_ >\n",
        "               sel_.estimator_.feature_importances_.mean())))"
      ],
      "metadata": {
        "id": "O_oe_5gl1IW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With SelectFromModel we use the mean of all importances as threshold. We can modify this threshold within the SelectFromModel if we want more or less features."
      ],
      "metadata": {
        "id": "BeZQqarR5oIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression"
      ],
      "metadata": {
        "id": "nD1-8hhf5xAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change only this line of code\n",
        "sel_ = SelectFromModel(RandomForestRegressor(n_estimators=100, random_state=10))"
      ],
      "metadata": {
        "id": "Ow6Wu-045RAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selecting features by using tree derived feature importance is a very straightforward, fast and generally accurate way of selecting good features for machine learning. In particular, if you are going to build tree methods.\n",
        "\n",
        "However, as I said, correlated features will show in a tree similar importance, but lower than compared to what their importance would be if the tree was built without the correlated counterparts.\n",
        "\n",
        "In situations like this, it is better to select features recursively, rather than altogether like we are doing in this lecture."
      ],
      "metadata": {
        "id": "MxuU7oMG5_O7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recursive Feature Selection using Random Forests importance\n"
      ],
      "metadata": {
        "id": "uprO4m1s6H2U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forests assign equal or similar importance to features that are highly correlated. In addition, when features are correlated, the importance assigned is lower than the importance attributed to the feature itself, should the tree be built without the correlated counterparts.\n",
        "\n",
        "Therefore, instead of eliminating features based on importance by brute force like we did in the previous notebook, we could get a better selection by removing one feature at a time, and recalculating the importance on each round. This procedure is called Recursive Feature Elimination (RFE)\n",
        "\n",
        "**RFE is a hybrid between embedded and wrapper methods: it is based on computation derived when fitting the model, but it also requires fitting several models.**\n",
        "\n",
        "**The cycle is as follows:**\n",
        "* Build Random Forests using all features\n",
        "* Remove least important feature\n",
        "* Build Random Forests and recalculate importance\n",
        "* Repeat until a criteria is met\n",
        "\n",
        "In this situation, when a feature that is highly correlated to another one is removed, then, the importance of the remaining feature increases. This may lead to a better feature space selection. On the downside, building several Random Forests is quite time and compute resource consuming, in particular if the dataset contains a high number of features.\n",
        "\n",
        "**Good method to select features**\n",
        "\n",
        "**Pros:**\n",
        "* More aggressive in reducing features by explicitly eliminating the least important ones.\n",
        "* Can be used with different types of models (not limited to Random Forest).\n",
        "* Gives more control over the number of features to keep, which is specified beforehand.\n",
        "\n",
        "**Cons:**\n",
        "* Computationally expensive because it requires re-fitting the model at each step.\n",
        "* The need to pre-specify the number of features to select is somewhat arbitrary.\n",
        "* Slower with large datasets compared to Random Forest importance."
      ],
      "metadata": {
        "id": "oNYNXj4d6TqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Step 1: Create a Random Forest model\n",
        "# We initialize the RandomForestClassifier with 10 trees and a fixed random seed for reproducibility.\n",
        "rf_model = RandomForestClassifier(n_estimators=10, random_state=10)\n",
        "\n",
        "# Step 2: Set up Recursive Feature Elimination (RFE)\n",
        "# RFE will use the Random Forest model to recursively eliminate features, one at a time.\n",
        "# The process will stop when 27 features are selected (n_features_to_select=27).\n",
        "sel_ = RFE(rf_model, n_features_to_select=27)\n",
        "\n",
        "# Step 3: Fit RFE to the training data\n",
        "# The RFE process begins by fitting the model to X_train and y_train,\n",
        "# removing the least important feature at each iteration until 27 features remain.\n",
        "sel_.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Get the selected features\n",
        "# After fitting, we get a boolean mask indicating which features were selected.\n",
        "# We use this mask to retrieve the names of the selected features from X_train.\n",
        "selected_feat = X_train.columns[(sel_.get_support())]\n",
        "\n",
        "# Step 5: Output the number of selected features\n",
        "# This line outputs how many features were selected by RFE.\n",
        "len(selected_feat)\n",
        "\n",
        "# Step 6: Display the names of the selected features\n",
        "# Finally, we display the list of feature names that were selected by RFE.\n",
        "selected_feat"
      ],
      "metadata": {
        "id": "7L5sTjTn6IgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additional notes\n",
        "\n",
        "**Random Forest Importance** is better when you want to quickly rank features and understand their relative importance across all features.\n",
        "\n",
        "**RFE** is more appropriate when you want to select a specific number of features and are willing to spend more computation time refining the feature set.\n",
        "\n",
        "* Higher Variance: For numerical features, we often prefer features with higher variance because they tend to carry more information. If a feature's values don't vary much (low variance), it might not contribute much to the model.\n",
        "\n",
        "* Lower Impurity: For classification problems, lower impurity means that the feature helps split the data in a way that the resulting groups are more pure, meaning they contain mostly one class. A low impurity means the feature is good at separating classes."
      ],
      "metadata": {
        "id": "HaA8N3kV-88Y"
      }
    }
  ]
}