{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Correlation\n",
        "\n",
        "Correlation Feature Selection evaluates subsets of features on the basis of the following hypothesis: \"Good feature subsets contain features highly correlated with the target, yet uncorrelated to each other\".\n",
        "\n",
        "**Methods**\n",
        "\n",
        "* The first one is a brute force function that finds correlated features without any further insight.\n",
        "\n",
        "* The second procedure finds groups of correlated features, which we can then explore to decide which one we keep and which ones we discard.\n",
        "\n",
        "**Common Question**\n",
        "\n",
        "If a group has several features that are highly correlated. Which one should we keep and which ones should we remove?\n",
        "\n",
        "One criteria to select which features to use from this group, would be to use those **with less missing data**. Our dataset contains no missing values, so this is not an option. But keep this in mind when you work with your own datasets.\n",
        "\n",
        "Alternatively, we could build a machine learning algorithm using all the features from the above list, and select the more predictive one.**"
      ],
      "metadata": {
        "id": "6-tZ39oJouA_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Correlation heatmap"
      ],
      "metadata": {
        "id": "EW_S32IVOE3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Build a correlation matrix for the features in X_train\n",
        "# Compute the Pearson correlation matrix for all features in X_train, which shows the linear relationships between them\n",
        "corrmat = X_train.corr(method='pearson')  # Pearson correlation is the default method in pandas' corr function\n",
        "\n",
        "# Step 2: Set up a color map for the heatmap\n",
        "# Create a diverging color palette with seaborn to visualize positive and negative correlations clearly\n",
        "cmap = sns.diverging_palette(220, 20, as_cmap=True)  # Customize the color gradient for the heatmap\n",
        "\n",
        "# Step 3: Configure the figure size\n",
        "# Create a figure and set its size to 11x11 inches for better visibility of the heatmap\n",
        "fig, ax = plt.subplots()   # Create the figure and axes\n",
        "fig.set_size_inches(11,11)  # Set the figure size\n",
        "\n",
        "# Step 4: Plot the correlation matrix using seaborn's heatmap\n",
        "# Visualize the correlation matrix as a heatmap with the custom color map\n",
        "sns.heatmap(corrmat, cmap=cmap)  # Plot the correlation matrix as a heatmap"
      ],
      "metadata": {
        "id": "88K_J2tyOX4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Brute Force Method\n",
        "\n",
        "In the brute force method, the approach is simple: you check every possible pair of features, and if two features are highly correlated (above a predefined threshold like 0.9), you remove one of them."
      ],
      "metadata": {
        "id": "t154WJbiicKn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The DropCorrelatedFeatures class from Feature-engine does a similar job to the brute force approach that we described earlier.\n",
        "\n",
        "The SmartCorrelationSelection allows us to select a feature from each correlated group based on model performance, number of missing values, cardinality or variance."
      ],
      "metadata": {
        "id": "-JuzRImejGdn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPBFOaSQh4bY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_validate\n",
        "\n",
        "from feature_engine.selection import DropCorrelatedFeatures, SmartCorrelatedSelection"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Set up the DropCorrelatedFeatures selector with the specified parameters\n",
        "# Initialize the DropCorrelatedFeatures object to identify and remove features\n",
        "# that have a correlation coefficient above the specified threshold (0.8)\n",
        "sel = DropCorrelatedFeatures(\n",
        "    threshold=0.8,      # Features with a Pearson correlation higher than 0.8 will be considered correlated\n",
        "    method='pearson',   # Use the Pearson correlation method to measure linear relationships between features\n",
        "    missing_values='ignore'  # Ignore missing values when calculating correlation\n",
        ")\n",
        "\n",
        "# Step 2: Fit the selector to the training data to find correlated features\n",
        "# Analyze the training data and identify pairs or groups of features that are highly correlated\n",
        "sel.fit(X_train)\n",
        "\n",
        "# Step 3: Retrieve sets of correlated features identified by the selector\n",
        "# Access the groups of correlated features; each set contains features that are strongly correlated with each other\n",
        "sel.correlated_feature_sets_\n",
        "\n",
        "# Step 4: Check the number of correlated features that will be removed\n",
        "# Get the count of features that will be dropped; only one feature from each correlated group is kept, others are removed\n",
        "len(sel.features_to_drop_)\n",
        "\n",
        "# Step 5: Remove correlated features from the training data\n",
        "# Drop the correlated features from the training dataset, keeping only one feature from each correlated group\n",
        "X_train = sel.transform(X_train)\n",
        "\n",
        "# Step 6: Remove correlated features from the test data using the same transformation\n",
        "# Apply the same transformation to the test dataset, ensuring consistency with the training data\n",
        "X_test = sel.transform(X_test)\n",
        "\n",
        "# Step 7: Display the shape of the training and test datasets after removing correlated features\n",
        "# Show the new dimensions of both training and test sets after removing correlated features\n",
        "X_train.shape, X_test.shape"
      ],
      "metadata": {
        "id": "lPxqyAn_jYN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grouping Method\n",
        "\n",
        "In the grouping method, instead of removing features pair by pair, you group all the correlated features together and then choose the most representative feature from each group.\n",
        "\n",
        "The second approach looks to identify groups of highly correlated features. And then, we can make further investigation within these groups to decide which feature we keep and which one we remove."
      ],
      "metadata": {
        "id": "uPeyd82OicMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SmartCorrelationSelection\n",
        "\n",
        "Model Performance\n",
        "We will keep a feature from each correlation group based on the performance of a random forest."
      ],
      "metadata": {
        "id": "FMZ57-2GkUPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Random Forest\n",
        "\n",
        "You train a Random Forest on the full dataset, which builds multiple decision trees. Each tree makes decisions based on feature splits that reduce impurity at each node.\n",
        "\n",
        "* **For classification, impurity is often measured using Gini impurity or entropy.**\n",
        "* **For regression, impurity is measured using variance.**\n"
      ],
      "metadata": {
        "id": "ZKElh8aYoPkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Set up the RandomForestClassifier with specified parameters\n",
        "# Initialize a RandomForestClassifier with 10 trees, a fixed random state for reproducibility, and parallel processing with 4 jobs\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=10,    # Number of trees in the forest\n",
        "    random_state=20,    # Seed for random number generation to ensure reproducibility\n",
        "    n_jobs=4,           # Number of parallel jobs to run for fitting the model\n",
        ")\n",
        "\n",
        "# Step 2: Set up the SmartCorrelatedSelection selector with the specified parameters\n",
        "# Initialize SmartCorrelatedSelection to identify and select features based on their correlation and model performance\n",
        "sel = SmartCorrelatedSelection(\n",
        "    variables=None,             # Consider all numerical variables for correlation analysis if set to None\n",
        "    method=\"pearson\",           # Use Pearson correlation to measure linear relationships between features\n",
        "    threshold=0.8,              # Correlation threshold above which features are considered correlated\n",
        "    missing_values=\"raise\",     # Raise an error if missing values are encountered\n",
        "    selection_method=\"model_performance\",  # Select features based on their impact on model performance\n",
        "    estimator=rf,               # RandomForestClassifier used to evaluate feature subsets\n",
        "    scoring=\"roc_auc\",          # Scoring metric for model performance evaluation (ROC AUC score)\n",
        "    cv=3,                       # Number of cross-validation folds for performance evaluation\n",
        ")\n",
        "\n",
        "# Step 3: Fit the SmartCorrelatedSelection selector to the training data\n",
        "# Train the SmartCorrelatedSelection by evaluating features in correlation groups with the RandomForestClassifier\n",
        "# This process may take some time due to multiple RandomForestClassifier trainings\n",
        "sel.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "2JYjBsLeif0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Examine the performance of the RandomForestClassifier for each feature in the second group of correlated features\n",
        "# Select the second group of correlated features identified by the SmartCorrelatedSelection\n",
        "group = sel.correlated_feature_sets_[1]\n",
        "\n",
        "# Step 5: Evaluate the RandomForestClassifier's performance for each feature in the selected group\n",
        "# Perform cross-validation for each feature in the group and print the mean ROC AUC score\n",
        "for f in group:\n",
        "\n",
        "    model = cross_validate(\n",
        "        rf,\n",
        "        X_train[f].to_frame(),   # Convert feature to DataFrame for model fitting\n",
        "        y_train,\n",
        "        cv=3,                    # Number of cross-validation folds\n",
        "        return_estimator=False,  # Do not return the trained models\n",
        "        scoring='roc_auc',       # Scoring metric for model performance evaluation (ROC AUC score)\n",
        "    )\n",
        "\n",
        "    print(f, model[\"test_score\"].mean())  # Print the feature name and its average ROC AUC score\n",
        "\n",
        "# Step 6: Check if specific features were retained or dropped by the selector\n",
        "# Check if 'var_28' was retained (not in the list of dropped features)\n",
        "'var_28' in sel.features_to_drop_\n",
        "\n",
        "# Check if 'var_5' was dropped (in the list of dropped features)\n",
        "'var_5' in sel.features_to_drop_\n",
        "\n",
        "# Check if 'var_75' was dropped (in the list of dropped features)\n",
        "'var_75' in sel.features_to_drop_"
      ],
      "metadata": {
        "id": "r1qYZi6OoHLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using variance"
      ],
      "metadata": {
        "id": "yfCA_nfKoT-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Set up the SmartCorrelatedSelection selector with the specified parameters\n",
        "# Initialize SmartCorrelatedSelection to identify and select features based on their correlation and variance\n",
        "sel = SmartCorrelatedSelection(\n",
        "    variables=None,             # Consider all numerical variables for correlation analysis if set to None\n",
        "    method=\"pearson\",           # Use Pearson correlation to measure linear relationships between features\n",
        "    threshold=0.8,              # Correlation threshold above which features are considered correlated\n",
        "    missing_values=\"raise\",     # Raise an error if missing values are encountered\n",
        "    selection_method=\"variance\", # Select features based on their variance within correlation groups\n",
        "    estimator=None,             # No estimator used for this selection method\n",
        "    scoring=\"roc_auc\",          # Scoring metric for model performance evaluation (ROC AUC score)\n",
        "    cv=3,                       # Number of cross-validation folds for performance evaluation\n",
        ")\n",
        "\n",
        "# Step 2: Fit the SmartCorrelatedSelection selector to the training data\n",
        "# Train the SmartCorrelatedSelection to identify features in correlation groups based on their variance\n",
        "sel.fit(X_train, y_train)\n",
        "\n",
        "# Step 3: Examine the variance of the features in the second group of correlated features\n",
        "# Select the second group of correlated features identified by the SmartCorrelatedSelection\n",
        "group = sel.correlated_feature_sets_[1]\n",
        "\n",
        "# Display the standard deviation (variance) of each feature in the selected group\n",
        "X_train[group].std()\n",
        "\n",
        "# Step 4: Check if specific features were retained or dropped by the selector\n",
        "# Check if 'var_28' was retained (not in the list of dropped features)\n",
        "'var_28' in sel.features_to_drop_\n",
        "\n",
        "# Check if 'var_5' was dropped (in the list of dropped features)\n",
        "'var_5' in sel.features_to_drop_\n",
        "\n",
        "# Check if 'var_75' was dropped (in the list of dropped features)\n",
        "'var_75' in sel.features_to_drop_"
      ],
      "metadata": {
        "id": "RUacsWKCoVOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Pipeline"
      ],
      "metadata": {
        "id": "TyekN3qExRlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from feature_engine.selection import (\n",
        "    DropConstantFeatures,\n",
        "    DropDuplicateFeatures,\n",
        "    SmartCorrelatedSelection,\n",
        ")"
      ],
      "metadata": {
        "id": "HROdHXlhxSz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Set up a pipeline with multiple feature selection methods\n",
        "# Stack the feature selection steps into a pipeline: remove constant features, duplicate features, and correlated features based on variance\n",
        "pipe = Pipeline([\n",
        "    ('constant', DropConstantFeatures(tol=0.998)),    # Remove quasi-constant features with a tolerance of 0.998\n",
        "    ('duplicated', DropDuplicateFeatures()),          # Remove duplicate features\n",
        "    ('correlation', SmartCorrelatedSelection(selection_method='variance')),  # Remove correlated features based on variance\n",
        "])\n",
        "\n",
        "# Step 2: Fit the pipeline to the training data\n",
        "# Apply the feature selection pipeline to the training data\n",
        "pipe.fit(X_train)\n",
        "\n",
        "# Step 3: Transform the training and test data by removing selected features\n",
        "# Remove the selected features from both training and test datasets using the fitted pipeline\n",
        "X_train = pipe.transform(X_train)  # Apply the transformations to the training data\n",
        "X_test = pipe.transform(X_test)    # Apply the same transformations to the test data\n",
        "\n",
        "# Step 4: Check the shapes of the transformed datasets (optional)\n",
        "X_train.shape, X_test.shape  # Display the shapes of the transformed datasets"
      ],
      "metadata": {
        "id": "mOwlpDJmRV_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_logistic(X_train, X_test, y_train, y_test):\n",
        "    # Step 1: Initialize the Logistic Regression model\n",
        "    # Create a logistic regression model with a fixed random state for reproducibility and a maximum of 500 iterations\n",
        "    logit = LogisticRegression(random_state=44, max_iter=500)\n",
        "\n",
        "    # Step 2: Fit the model to the training data\n",
        "    # Train the logistic regression model using the training data\n",
        "    logit.fit(X_train, y_train)\n",
        "\n",
        "    # Step 3: Evaluate the model on the training set\n",
        "    # Predict probabilities for the training set and compute the ROC-AUC score\n",
        "    print('Train set')\n",
        "    pred = logit.predict_proba(X_train)  # Get predicted probabilities for the training data\n",
        "    print('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_train, pred[:, 1])))  # ROC-AUC score for training set\n",
        "\n",
        "    # Step 4: Evaluate the model on the test set\n",
        "    # Predict probabilities for the test set and compute the ROC-AUC score\n",
        "    print('Test set')\n",
        "    pred = logit.predict_proba(X_test)  # Get predicted probabilities for the test data\n",
        "    print('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_test, pred[:, 1])))  # ROC-AUC score for test set"
      ],
      "metadata": {
        "id": "X4gZ373BRjQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Standardize the training data\n",
        "# Fit a StandardScaler to the training data to standardize features by removing the mean and scaling to unit variance\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "\n",
        "# Step 2: Apply the scaling to both the training and test data\n",
        "# Use the fitted scaler to transform (standardize) both the training and test data before feeding them to the logistic regression model\n",
        "run_logistic(scaler.transform(X_train),  # Standardize X_train and pass to logistic regression\n",
        "             scaler.transform(X_test),   # Standardize X_test and pass to logistic regression\n",
        "             y_train, y_test)            # Pass target variables y_train and y_test as they are"
      ],
      "metadata": {
        "id": "lUbuavkrRky7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First Code** (Impurity Approach): Uses Random Forest to select features based on their impurity reduction and model performance (e.g., ROC AUC score). It retains features that contribute most to reducing impurity and improving predictive power.\n",
        "\n",
        "**Second Code** (Variance Approach): Uses variance within correlated groups to select features. It retains the feature with the highest variance in each group, as high variance indicates greater differentiation between data points.\n",
        "\n",
        "**Why Select High Variance Features?**\n",
        "\n",
        "* A feature with higher variance generally contains more information about differences between data points. For example, if one feature has many unique or varying values, it is more likely to be useful for distinguishing between different outcomes or classes in a model.\n",
        "* Correlated features often carry similar information. Selecting the feature with the highest variance helps ensure you are keeping the one that best represents the differences in the data, while discarding the others that are largely redundant."
      ],
      "metadata": {
        "id": "sE0Q3Z5fWpZX"
      }
    }
  ]
}